# -*- coding: utf-8 -*-
"""Day25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11i1UnTQ7cIwgkFiHrPhBrdF-IbYFR8Jl
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

dataset=pd.read_csv("Mall_Customers.csv")
dataset.head()

dataset.isna().sum()

X=dataset.iloc[:,3:].values
X

import scipy.cluster.hierarchy as sch
dendrogram=sch.dendrogram(sch.linkage(X,method='ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()

from sklearn.cluster import AgglomerativeClustering
hc=AgglomerativeClustering(n_clusters=5,metric='euclidean',linkage='ward')
y_hc=hc.fit_predict(X)
y_hc

plt.scatter(X[y_hc==0,0],X[y_hc==0,1],s=100,c='red',label='cluster 1')
plt.scatter(X[y_hc==1,0],X[y_hc==1,1],s=100,c='blue',label='cluster 2')
plt.scatter(X[y_hc==2,0],X[y_hc==2,1],s=100,c='purple',label='cluster 3')
plt.scatter(X[y_hc==3,0],X[y_hc==3,1],s=100,c='violet',label='cluster 4')
plt.scatter(X[y_hc==4,0],X[y_hc==4,1],s=100,c='yellow',label='cluster 5')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.title('Hierarchical Clustering')
plt.legend()
plt.show()

from sklearn.metrics import silhouette_score
score=silhouette_score(X,y_hc)
print('Silhouette Score for Hierarchical Clustering',score)

# Load dataset with correct encoding
df = pd.read_csv("/content/all-data.csv", encoding="latin1")

print("Dataset Loaded Successfully âœ…")
print(df.head())
print("\nColumns:", df.columns)

# If column name is different, adjust here
text_column = df.columns[1]   # usually second column contains news text

news_text = df[text_column]

print("\nSample News:")
print(news_text.head())

vectorizer = TfidfVectorizer(
    stop_words='english',
    max_features=1000
)

X = vectorizer.fit_transform(news_text)

print("TF-IDF Shape:", X.shape)

subset = X[:200].toarray()   # first 200 articles

linked = linkage(subset, method='ward')

plt.figure(figsize=(12, 6))
dendrogram(linked)
plt.title("Dendrogram for Financial News")
plt.xlabel("Articles")
plt.ylabel("Distance")
plt.show()

model = AgglomerativeClustering(
    n_clusters=4,
    linkage='ward'
)

clusters = model.fit_predict(X.toarray())

df["Cluster"] = clusters

print("\nCluster Distribution:")
print(df["Cluster"].value_counts())

for i in range(4):
    print(f"\nCluster {i} Sample Articles:")
    print(df[df["Cluster"] == i][text_column].head(5))

terms = vectorizer.get_feature_names_out()

for i in range(4):
    cluster_text = df[df["Cluster"] == i][text_column]
    temp_vector = vectorizer.transform(cluster_text)
    mean_tfidf = np.mean(temp_vector.toarray(), axis=0)
    top_indices = mean_tfidf.argsort()[-10:]
    top_words = [terms[index] for index in top_indices]

    print(f"\nCluster {i} Top Words:")
    print(top_words)

from sklearn.metrics import silhouette_score

for k in range(2, 8):
    model = AgglomerativeClustering(n_clusters=k, linkage='ward')
    labels = model.fit_predict(X.toarray())
    score = silhouette_score(X, labels)
    print(f"Clusters: {k}, Silhouette Score: {score}")

import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt
import numpy as np

# Convert sparse matrix to dense
X_dense = X.toarray()

# Use subset for speed
X_subset = X_dense[:200]

# Create linkage matrix
Z = sch.linkage(X_subset, method='ward')

plt.figure(figsize=(12,6))

# Plot dendrogram
dendro = sch.dendrogram(Z)

# Get maximum height of dendrogram
max_height = np.max(Z[:, 2])

# Compute median (middle height)
median_height = max_height / 2

# Draw horizontal line exactly at median
plt.axhline(y=median_height, color='r', linestyle='--')

plt.title('Dendrogram (Cut at Median Height)')
plt.xlabel('Articles')
plt.ylabel('Euclidean Distance')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt # For visualization
from sklearn.cluster import DBSCAN # DBSCAN clustering algorithm
from sklearn.preprocessing import StandardScaler # Feature scaling

dataset2=pd.read_csv("Mall_Customers.csv")
dataset2.head()

X = dataset[['Annual Income (k$)', 'Spending Score (1-100)']]

X

plt.figure(figsize=(7, 5))

plt.scatter(
    X['Annual Income (k$)'],
    X['Spending Score (1-100)']
)

plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.title('Annual Income vs Spending Score')

plt.show()

scalar=StandardScaler()
X_scaled=scalar.fit_transform(X)

dbscan=DBSCAN(eps=0.3,min_samples=5)
clusters=dbscan.fit_predict(X_scaled)

plt.figure(figsize=(7,5))

# Plot all clustered points
plt.scatter(
    X[:, 0],   # Annual Income column
    X[:, 1],   # Spending Score column
    c=clusters,
    cmap='rainbow'
)

# Highlight Noise points (for DBSCAN where cluster = -1)
noise_points = clusters == -1

plt.scatter(
    X[noise_points, 0],
    X[noise_points, 1],
    color='black',
    label='Noise'
)

plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.title('Customer Segmentation')
plt.legend()
plt.show()

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
import numpy as np

eps_values = [0.2, 0.3, 8.5]

for eps in eps_values:
    print(f"\nEvaluating DBSCAN using eps = {eps}")

    # Apply DBSCAN
    dbscan = DBSCAN(eps=eps, min_samples=5)
    labels = dbscan.fit_predict(X_scaled)

    # Number of clusters (ignore noise label -1)
    unique_labels = set(labels)
    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)

    # Count noise points
    n_noise = list(labels).count(-1)
    noise_ratio = n_noise / len(labels)

    print("Number of Clusters:", n_clusters)
    print("Number of Noise Points:", n_noise)
    print("Noise Ratio:", round(noise_ratio, 2))

    # Silhouette Score (only if valid)
    if n_clusters > 1:
        X_non_noise = X_scaled[labels != -1]
        labels_non_noise = labels[labels != -1]

        score = silhouette_score(X_non_noise, labels_non_noise)
        print("Silhouette Score:", round(score, 3))
    else:
        print("Silhouette Score: Not applicable (Need at least 2 clusters)")