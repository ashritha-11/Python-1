# -*- coding: utf-8 -*-
"""Day23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WuHhASvRur1_DhDzWx6YRE-mGp_ayHAP
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#getting dataset from sklearn
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

cancer['feature_names']

cancer['data']

dataset = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns=np.append(cancer['feature_names'], ['target']))
dataset.head()

dataset.isna().sum()

#creating feature matrix and dependent variable vector
X = dataset.iloc[: , :- 1].values
y = dataset.iloc[: , -1].values

#training set and test set
from sklearn.model_selection import train_test_split
x_train , x_test , y_train ,y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test =sc.transform(x_test)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=25 ,metric = 'minkowski')
classifier.fit(x_train , y_train)

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
metric_params=None, n_jobs=None, n_neighbors=25, p=2,
weights='uniform')

y_pred = classifier.predict(x_test)
y_pred

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
display(cm)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
display(f"Accuracy Score: {accuracy}")
sns.heatmap(cm, annot=True)

import pandas as pd

# Load dataset
df = pd.read_csv("credit_risk_dataset.csv")

# View first 5 rows
df.head()

df.info()

list(df.columns)

df[['person_age',
    'person_income',
    'loan_amnt',
    'cb_person_cred_hist_length']].describe()

import matplotlib.pyplot as plt

plt.hist(df['person_age'], bins=10)
plt.xlabel("Age")
plt.ylabel("Number of Customers")
plt.title("Age Distribution")
plt.show()

plt.hist(df['person_income'], bins=10)
plt.xlabel("Income")
plt.ylabel("Number of Customers")
plt.title("Income Distribution")
plt.show()

plt.hist(df['loan_amnt'], bins=10)
plt.xlabel("Loan Amount")
plt.ylabel("Number of Customers")
plt.title("Loan Amount Distribution")
plt.show()

plt.hist(df['cb_person_cred_hist_length'], bins=10)
plt.xlabel("Credit History Length (years)")
plt.ylabel("Number of Customers")
plt.title("Credit History Length Distribution")
plt.show()

similarity_features = [
    'person_age',
    'person_income',
    'loan_amnt',
    'cb_person_cred_hist_length'
]

df['cb_person_default_on_file'] = df['cb_person_default_on_file'].map({'Y': 1, 'N': 0})

similarity_features.append('cb_person_default_on_file')

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[similarity_features])

scaled_df = pd.DataFrame(scaled_data, columns=similarity_features)
scaled_df.head()

features = [
    'person_age',
    'person_income',
    'loan_amnt',
    'cb_person_cred_hist_length',
    'cb_person_default_on_file'
]

# Convert Y/N to 1/0
df['cb_person_default_on_file'] = df['cb_person_default_on_file'].map({'Y': 1, 'N': 0})

X = df[features]
y = df['loan_status']   # Target (0 = Low Risk, 1 = High Risk)

X.isnull().sum()

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

y_pred = knn.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Very small K
knn_k1 = KNeighborsClassifier(n_neighbors=1)
knn_k1.fit(X_train, y_train)

y_pred_k1 = knn_k1.predict(X_test)

print("Accuracy for K = 1:", accuracy_score(y_test, y_pred_k1))

knn_k5 = KNeighborsClassifier(n_neighbors=5)
knn_k5.fit(X_train, y_train)

y_pred_k5 = knn_k5.predict(X_test)

print("Accuracy for K = 5:", accuracy_score(y_test, y_pred_k5))

knn_k50 = KNeighborsClassifier(n_neighbors=50)
knn_k50.fit(X_train, y_train)

y_pred_k50 = knn_k50.predict(X_test)

print("Accuracy for K = 50:", accuracy_score(y_test, y_pred_k50))

accuracy_scores = []

K_values = range(1, 31)

for k in K_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy_scores.append(accuracy_score(y_test, y_pred))

import matplotlib.pyplot as plt

plt.plot(K_values, accuracy_scores)
plt.xlabel("K (Number of Neighbors)")
plt.ylabel("Accuracy")
plt.title("Effect of K on KNN Accuracy")
plt.show()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

accuracy_scores = {}
K_values = range(1, 31)

for k in K_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy_scores[k] = accuracy_score(y_test, y_pred)

best_k = max(accuracy_scores, key=accuracy_scores.get)
best_accuracy = accuracy_scores[best_k]

print("Best K:", best_k)
print("Best Accuracy:", best_accuracy)

import matplotlib.pyplot as plt

plt.plot(accuracy_scores.keys(), accuracy_scores.values())
plt.xlabel("Number of Neighbors (K)")
plt.ylabel("Accuracy")
plt.title("Choosing Optimal K for KNN")
plt.show()

from sklearn.neighbors import KNeighborsClassifier

final_knn = KNeighborsClassifier(n_neighbors=best_k)
final_knn.fit(X_train, y_train)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

best_k = 5   # default balanced choice
final_knn = KNeighborsClassifier(n_neighbors=best_k)
final_knn.fit(X_train, y_train)

y_pred_final = final_knn.predict(X_test)

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

cm = confusion_matrix(y_test, y_pred_final)

plt.figure(figsize=(5,4))
plt.imshow(cm)
plt.title("Confusion Matrix - KNN Credit Risk Model")
plt.colorbar()

classes = ['Low Risk', 'High Risk']
ticks = np.arange(len(classes))
plt.xticks(ticks, classes)
plt.yticks(ticks, classes)

for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha="center", va="center")

plt.xlabel("Predicted Label")
plt.ylabel("Actual Label")
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

model = RandomForestClassifier(random_state=42)

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt']
}

grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3,
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

# -----------------------------
# Import Libraries
# -----------------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# -----------------------------
# Load Dataset
# -----------------------------
df = pd.read_csv("Social_Network_Ads.csv")

# -----------------------------
# Feature Selection
# -----------------------------
# Using Age and EstimatedSalary
X = df.iloc[:, [2, 3]].values
y = df.iloc[:, -1].values

# -----------------------------
# Train-Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -----------------------------
# Feature Scaling
# -----------------------------
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# -----------------------------
# AdaBoost Model
# -----------------------------
base_model = DecisionTreeClassifier(max_depth=1)

classifier = AdaBoostClassifier(
    estimator=base_model,
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)

# Train model
classifier.fit(X_train, y_train)

# -----------------------------
# Predictions
# -----------------------------
y_pred = classifier.predict(X_test)

# -----------------------------
# Accuracy
# -----------------------------
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# -----------------------------
# Confusion Matrix
# -----------------------------
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# -----------------------------
# Plot Confusion Matrix
# -----------------------------
plt.figure(figsize=(6, 4))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=["Not Purchased", "Purchased"],
    yticklabels=["Not Purchased", "Purchased"]
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - AdaBoost Classifier")
plt.show()

# -----------------------------
# Import Libraries
# -----------------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# -----------------------------
# Load Dataset
# -----------------------------
df = pd.read_csv("Social_Network_Ads.csv")

# -----------------------------
# Feature Selection
# -----------------------------
# Using Age and EstimatedSalary
X = df.iloc[:, [2, 3]].values
y = df.iloc[:, -1].values

# -----------------------------
# Train-Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -----------------------------
# Feature Scaling
# -----------------------------
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# -----------------------------
# Gradient Boosting Model
# -----------------------------
classifier = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

# Train model
classifier.fit(X_train, y_train)

# -----------------------------
# Predictions
# -----------------------------
y_pred = classifier.predict(X_test)

# -----------------------------
# Accuracy
# -----------------------------
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# -----------------------------
# Confusion Matrix
# -----------------------------
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# -----------------------------
# Plot Confusion Matrix
# -----------------------------
plt.figure(figsize=(6, 4))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Greens",
    xticklabels=["Not Purchased", "Purchased"],
    yticklabels=["Not Purchased", "Purchased"]
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Gradient Boosting Classifier")
plt.show()

# -----------------------------
# Import Libraries
# -----------------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix

from xgboost import XGBClassifier

# -----------------------------
# Load Dataset
# -----------------------------
df = pd.read_csv("Social_Network_Ads.csv")

# -----------------------------
# Feature Selection
# -----------------------------
# Using Age and EstimatedSalary
X = df.iloc[:, [2, 3]].values
y = df.iloc[:, -1].values

# -----------------------------
# Train-Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -----------------------------
# Feature Scaling
# -----------------------------
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# -----------------------------
# XGBoost Model
# -----------------------------
classifier = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# Train model
classifier.fit(X_train, y_train)

# -----------------------------
# Predictions
# -----------------------------
y_pred = classifier.predict(X_test)

# -----------------------------
# Accuracy
# -----------------------------
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# -----------------------------
# Confusion Matrix
# -----------------------------
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# -----------------------------
# Plot Confusion Matrix
# -----------------------------
plt.figure(figsize=(6, 4))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Oranges",
    xticklabels=["Not Purchased", "Purchased"],
    yticklabels=["Not Purchased", "Purchased"]
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - XGBoost Classifier")
plt.show()

# =====================================================
# TELCO CUSTOMER CHURN – BASELINE + ENSEMBLE MODEL
# =====================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    classification_report,
    recall_score,
    precision_score,
    f1_score
)

# =====================================================
# LOAD DATASET
# =====================================================
df = pd.read_csv("Telco-Customer-Churn.csv")

# =====================================================
# DATA PREPROCESSING
# =====================================================

# Drop customer ID (not useful for prediction)
df.drop("customerID", axis=1, inplace=True)

# Convert TotalCharges to numeric (handles spaces)
df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors="coerce")

# Fill missing values
df["TotalCharges"].fillna(df["TotalCharges"].median(), inplace=True)

# Encode target variable (business meaning: 1 = churn)
df["Churn"] = df["Churn"].map({"Yes": 1, "No": 0})

# Encode categorical features
label_encoder = LabelEncoder()
for col in df.select_dtypes(include="object").columns:
    df[col] = label_encoder.fit_transform(df[col])

# =====================================================
# FEATURE–TARGET SPLIT
# =====================================================
X = df.drop("Churn", axis=1)
y = df["Churn"]

# =====================================================
# TRAIN–TEST SPLIT
# =====================================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# =====================================================
# BASELINE MODEL – DECISION TREE
# =====================================================
baseline_model = DecisionTreeClassifier(
    max_depth=5,
    random_state=42
)

baseline_model.fit(X_train, y_train)

y_pred_base = baseline_model.predict(X_test)

print("\n========== BASELINE MODEL (DECISION TREE) ==========")
print("Accuracy:", accuracy_score(y_test, y_pred_base))
print("Recall (Churn):", recall_score(y_test, y_pred_base))

cm_base = confusion_matrix(y_test, y_pred_base)
plt.figure(figsize=(5,4))
sns.heatmap(cm_base, annot=True, fmt="d", cmap="Blues")
plt.title("Baseline Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# =====================================================
# ENSEMBLE MODEL – GRADIENT BOOSTING
# =====================================================
ensemble_model = GradientBoostingClassifier(
    n_estimators=150,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

ensemble_model.fit(X_train, y_train)

y_pred_ens = ensemble_model.predict(X_test)

print("\n========== ENSEMBLE MODEL (GRADIENT BOOSTING) ==========")
print("Accuracy:", accuracy_score(y_test, y_pred_ens))
print("Precision:", precision_score(y_test, y_pred_ens))
print("Recall:", recall_score(y_test, y_pred_ens))
print("F1-Score:", f1_score(y_test, y_pred_ens))

print("\nClassification Report:\n")
print(classification_report(y_test, y_pred_ens))

cm_ens = confusion_matrix(y_test, y_pred_ens)
plt.figure(figsize=(5,4))
sns.heatmap(cm_ens, annot=True, fmt="d", cmap="Greens")
plt.title("Ensemble Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# =====================================================
# FEATURE IMPORTANCE
# =====================================================
feature_importance = pd.Series(
    ensemble_model.feature_importances_,
    index=X.columns
).sort_values(ascending=False)

print("\nTop 10 Important Features:\n")
print(feature_importance.head(10))

plt.figure(figsize=(10,4))
feature_importance.head(10).plot(kind="bar")
plt.title("Top Features Contributing to Churn")
plt.ylabel("Importance Score")
plt.tight_layout()
plt.show()