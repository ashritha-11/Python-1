# -*- coding: utf-8 -*-
"""Day20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HqQge_h_v9-yMzT5ubDfdOc5sTNL1K43
"""

import numpy as np
import pandas as pd
import seaborn as sns

"""The goal is to predict whether a user will purchase a product based on age and
estimated salary.
"""

# Load dataset
df = pd.read_csv("Social_Network_Ads.csv")

# View first few rows
df.head()

# Dataset shape
print("Shape:", df.shape)

# Column info
df.info()

# Check missing values
df.isnull().sum()

x=df.iloc[: ,2:4].values
y=df.iloc[:,-1].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.tree import DecisionTreeClassifier

classifier = DecisionTreeClassifier(
    criterion="entropy"

)

classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy Score:", accuracy)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

TN, FP, FN, TP = cm.ravel()

print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

sns.heatmap(cm, annot=True, fmt="d")

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(18, 10))
plot_tree(
classifier,
feature_names=["Age", "Estimated Salary"],
class_names=["Not Purchased", "Purchased"],
filled=True,
rounded=True
)
plt.show()

from sklearn.tree import DecisionTreeClassifier

classifier = DecisionTreeClassifier(
    criterion="entropy",
    max_depth=4

)

classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy Score:", accuracy)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

TN, FP, FN, TP = cm.ravel()

print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

sns.heatmap(cm, annot=True, fmt="d")

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(18, 10))
plot_tree(
classifier,
feature_names=["Age", "Estimated Salary"],
class_names=["Not Purchased", "Purchased"],
filled=True,
rounded=True
)
plt.show()

"""Using Titanic passenger data, our goal is to predict whether a passenger survived
or not based on their details.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import plot_tree

df = sns.load_dataset("titanic")

# View first 5 rows
df.head()

df.isnull().sum()

df = df[["survived", "pclass", "sex", "age", "fare", "embarked"]]
df.head()

df.isna().sum()

df["age"].fillna(df["age"].median(), inplace=True)
df["embarked"].fillna(df["embarked"].mode()[0], inplace=True)

encoder = LabelEncoder()

df["sex"] = encoder.fit_transform(df["sex"])
df["embarked"] = encoder.fit_transform(df["embarked"])

df.head()

X = df.drop("survived", axis=1)
y = df["survived" ]

X.head(), y.head()

model = DecisionTreeClassifier(
criterion="gini",
max_depth=4,
random_state=42

)

model.fit(X, y)

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(20, 10))
plot_tree(
    model,
    feature_names=X.columns,      # automatically uses feature names
    class_names=["Not Survived", "Survived"],
    filled=True

)
plt.title("Decision Tree (Gini Index, max_depth=4)")
plt.show()

new_passenger = pd.DataFrame([[3, 1, 25, 7, 2]], columns=X.columns)

prediction = model.predict(new_passenger)
prediction

if prediction[0] == 1:
      print("Passenger is likely to Survive")
else:
      print("Passenger is likely NOT to Survive ")

# -----------------------------
# Bank Marketing Prediction using Decision Tree (Gini) - With Interpretation
# -----------------------------
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt

# -----------------------------
# 1. Load Dataset
# -----------------------------
df = pd.read_csv("bank_marketing_dataset.csv")

# -----------------------------
# 2. Identify numerical and categorical columns
# -----------------------------
num_cols = df.select_dtypes(exclude="object").columns  # numeric features
cat_cols = df.select_dtypes(include="object").columns.drop("deposit")  # categorical features excluding target

# -----------------------------
# 3. Remove Outliers using IQR
# -----------------------------
def remove_outliers_iqr(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

df_clean = remove_outliers_iqr(df, num_cols)
print(f"Original dataset size: {df.shape[0]}, After outlier removal: {df_clean.shape[0]}")

# -----------------------------
# 4. Define Features and Target
# -----------------------------
X = df_clean.drop("deposit", axis=1)
y = df_clean["deposit"].map({"no": 0, "yes": 1})  # 0=Not Likely, 1=Likely

# -----------------------------
# 5. Preprocessing: One-Hot Encode categorical features
# -----------------------------
preprocessor = ColumnTransformer([
    ("num", "passthrough", num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols)
])

X_transformed = preprocessor.fit_transform(X)
feature_names = num_cols.tolist() + list(preprocessor.named_transformers_["cat"].get_feature_names_out(cat_cols))

# -----------------------------
# 6. Train-Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_transformed, y, test_size=0.2, random_state=42, stratify=y
)

# -----------------------------
# 7. Train Decision Tree
# -----------------------------
dtree = DecisionTreeClassifier(
    criterion="gini",
    max_depth=6,
    min_samples_leaf=5,
    class_weight="balanced",
    random_state=42
)
dtree.fit(X_train, y_train)

# -----------------------------
# 8. Predictions and Evaluation
# -----------------------------
y_pred = dtree.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("\nAccuracy:", round(accuracy*100,2), "%")

cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", cm)

TN, FP, FN, TP = cm.ravel()
print("\nTP:", TP, "TN:", TN, "FP:", FP, "FN:", FN)

print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=["Not Likely", "Likely"]))

# -----------------------------
# 9. Visualize Decision Tree
# -----------------------------
plt.figure(figsize=(20,10))
plot_tree(dtree, feature_names=feature_names, class_names=["Not Likely", "Likely"],
          filled=True, rounded=True, fontsize=10)
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import plot_tree

# -----------------------------
# Load Dataset
# -----------------------------
df = pd.read_csv("Social_Network_Ads.csv")

# -----------------------------
# Features & Target
# -----------------------------
X = df.iloc[:, 2:4].values   # Age, EstimatedSalary
Y = df.iloc[:, -1].values   # Purchased

# -----------------------------
# Train-Test Split
# -----------------------------
x_train, x_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=42
)

# -----------------------------
# Feature Scaling
# -----------------------------
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

# -----------------------------
# Random Forest Classifier
# -----------------------------
classifier = RandomForestClassifier(
    n_estimators=100,       # increased for better accuracy
    criterion='entropy',
    random_state=42
)

classifier.fit(x_train, Y_train)

# -----------------------------
# Prediction & Accuracy
# -----------------------------
Y_pred = classifier.predict(x_test)

accuracy = accuracy_score(Y_test, Y_pred)
print("Accuracy:", round(accuracy * 100, 2), "%")

single_tree = classifier.estimators_[0]

plt.figure(figsize=(18, 10))
plot_tree(
    single_tree,
    feature_names=["Age", "EstimatedSalary"],
    class_names=["Not Purchased", "Purchased"],
    filled=True,
    rounded=True
)
plt.show()

df = pd.read_csv("creditcard.csv")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.tree import plot_tree

# -----------------------------
# 1. Load Dataset
# -----------------------------
df = pd.read_csv("creditcard.csv")

# Remove rows where target is NaN
df = df.dropna(subset=["Class"])

# -----------------------------
# 2. Features and Target
# -----------------------------
X = df.drop("Class", axis=1).values
Y = df["Class"].values

# -----------------------------
# 3. Train-Test Split
# -----------------------------
x_train, x_test, Y_train, Y_test = train_test_split(
    X, Y,
    test_size=0.2,
    random_state=42,
    stratify=Y
)

# -----------------------------
# 4. Feature Scaling
# -----------------------------
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

# -----------------------------
# 5. Random Forest Classifier
# -----------------------------
classifier = RandomForestClassifier(
    n_estimators=100,
    criterion="gini",
    random_state=42,
    class_weight="balanced"
)

classifier.fit(x_train, Y_train)

# -----------------------------
# 6. Predictions
# -----------------------------
Y_pred = classifier.predict(x_test)

# -----------------------------
# 7. Evaluation
# -----------------------------
print("Accuracy:", round(accuracy_score(Y_test, Y_pred) * 100, 2), "%")
print("\nConfusion Matrix:")
print(confusion_matrix(Y_test, Y_pred))

print("\nClassification Report:")
print(classification_report(Y_test, Y_pred, target_names=["Normal", "Fraud"]))

single_tree = classifier.estimators_[0]

plt.figure(figsize=(20, 10))
plot_tree(
    single_tree,
    feature_names=df.drop("Class", axis=1).columns,
    class_names=["Normal", "Fraud"],
    filled=True,
    rounded=True,
    max_depth=3   # keep tree readable
)
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# -----------------------------
# 1. Load Dataset
# -----------------------------
df = pd.read_csv("creditcard.csv")

# Remove rows where target is NaN
df = df.dropna(subset=["Class"])

# -----------------------------
# 2. Features and Target (same variable names)
# -----------------------------
X = df.drop("Class", axis=1).values
Y = df["Class"].values

# -----------------------------
# 3. Train-Test Split
# -----------------------------
x_train, x_test, Y_train, Y_test = train_test_split(
    X, Y,
    test_size=0.2,
    random_state=42,
    stratify=Y
)

# -----------------------------
# 4. Feature Scaling
# -----------------------------
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

# -----------------------------
# 5. Decision Tree Classifier
# -----------------------------
classifier = DecisionTreeClassifier(
    criterion="gini",
    max_depth=4,            # controls overfitting
    min_samples_split=50,   # improves generalization
    random_state=42,
    class_weight="balanced"
)

classifier.fit(x_train, Y_train)

# -----------------------------
# 6. Predictions
# -----------------------------
Y_pred = classifier.predict(x_test)

# -----------------------------
# 7. Evaluation
# -----------------------------
print("Accuracy:", round(accuracy_score(Y_test, Y_pred) * 100, 2), "%")

print("\nConfusion Matrix:")
print(confusion_matrix(Y_test, Y_pred))

print("\nClassification Report:")
print(classification_report(Y_test, Y_pred, target_names=["Normal", "Fraud"]))

plt.figure(figsize=(22, 12))
plot_tree(
    classifier,
    feature_names=df.drop("Class", axis=1).columns,
    class_names=["Normal", "Fraud"],
    filled=True,
    rounded=True,
    max_depth=3   # for readability
)
plt.show()

