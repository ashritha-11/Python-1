# -*- coding: utf-8 -*-
"""day18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-rgsn3LvBukTRtWBCXTmcUusFc8GB1pR
"""

import seaborn as sns
import pandas as pd
df = sns.load_dataset('taxis')
print(df.head())
print(df.shape)
df.describe()
df.describe(include='object')

print(df.isna().sum())
print((df.isna().sum() / df.shape[0]) * 100)

df = df.dropna()
df = df.reset_index(drop=True)
print(df.shape)

print(df['color'].value_counts())
yellow_tip = df['tip'][df['color'] == 'yellow']
green_tip = df['tip'][df['color'] == 'green']
print("Green tip shape:", green_tip.shape)
print("Yellow tip shape:", yellow_tip.shape)

from scipy import stats
t_stat, p_val = stats.ttest_ind(yellow_tip, green_tip)

print("P-value:", p_val)
alpha = 0.05
if p_val < alpha:
    print("Reject null hypothesis: The tip for yellow category is different than others.")
else:
    print("Fail to reject null hypothesis: No significant difference found.")

import seaborn as sns
import pandas as pd
from scipy import stats
df = sns.load_dataset('taxis')
missing_pct = ((df.shape[0] - df.dropna().shape[0]) / df.shape[0]) * 100
print(f"Percentage of missing data: {missing_pct}%")
df = df.dropna()
df = df.reset_index(drop=True)
print(f"New shape after cleaning: {df.shape}")
print(df['color'].value_counts())
sns.countplot(x=df['pickup_borough'], palette='Set1')
print(df['tip'].groupby(df['pickup_borough']).std())

# 3. INDEPENDENT T-TEST (Yellow vs Green Taxis)

yellow_tip = df['tip'][df['color'] == 'yellow']
green_tip = df['tip'][df['color'] == 'green']

print(f"Green samples: {green_tip.shape}") # (968,)
print(f"Yellow samples: {yellow_tip.shape}") # (5373,)

t_stat, p_val_ttest = stats.ttest_ind(yellow_tip, green_tip)
print(f"T-Test P-value: {p_val_ttest}")

alpha = 0.05
if p_val_ttest < alpha:
    print("Reject null hypothesis: The tip for yellow category is different than others.")
else:
    print("Fail to reject null hypothesis")

# 4. ANOVA TEST (Comparing Multiple Boroughs)
manhattan_tip = df['tip'][df['pickup_borough'] == 'Manhattan']
queens_tip = df['tip'][df['pickup_borough'] == 'Queens']
bronx_tip = df['tip'][df['pickup_borough'] == 'Bronx']
brooklyn_tip = df['tip'][df['pickup_borough'] == 'Brooklyn']

# Perform One-way ANOVA
categories = [group['tip'].values for name, group in df.groupby('pickup_borough')]
f_stat, p_val_anova = stats.f_oneway(*categories)

print(f"ANOVA P-value: {p_val_anova}")
if p_val_anova < alpha:
    print("Reject null hypothesis: Significant difference in tips between boroughs.")

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from scipy import stats

# 1. LOAD DATA
df = sns.load_dataset('taxis')
df_raw = df.copy()

# 2. DATA CLEANING
df = df.dropna().reset_index(drop=True)
sns.set_theme(style="whitegrid")

# PLOT 1: Missing Values (Pre-cleaning)
plt.figure(figsize=(10, 5))
sns.barplot(x=df_raw.isna().sum().index, y=df_raw.isna().sum().values, palette="Reds_r")
plt.title("Missing Values Count per Column")
plt.xticks(rotation=45)
plt.ylabel("Count")
plt.show()

# PLOT 2: Distribution of Taxi Colors (Categorical Analysis)
# Based on df.color.value_counts()
plt.figure(figsize=(7, 5))
sns.countplot(data=df, x='color', palette=['yellow', 'green'], edgecolor='black')
plt.title("Sample Size Comparison: Yellow vs Green Taxis")
plt.show()

# PLOT 3: Tip Distribution (KDE Plot)
# This provides the visual logic for your Independent T-Test

plt.figure(figsize=(10, 6))
sns.kdeplot(data=df, x='tip', hue='color', fill=True, palette=['gold', 'seagreen'], common_norm=False)
plt.title("KDE Plot: Tip Amount Distribution by Taxi Color")
plt.xlim(0, 10)
plt.show()

# --- STATISTICAL TEST RESULTS ---
yellow_tip = df[df['color'] == 'yellow']['tip']
green_tip = df[df['color'] == 'green']['tip']
t_stat, p_val = stats.ttest_ind(yellow_tip, green_tip)

print(f"T-Test Result: P-value = {p_val}")
if p_val < 0.05:
    print("Result: Statistically Significant Difference found.")

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from scipy import stats

# 1. LOAD DATA
df_raw = sns.load_dataset('taxis')
df = df_raw.copy()

# 2. DATA CLEANING (Based on your screenshots)
# Check nulls and drop them
df = df.dropna().reset_index(drop=True)


# ---------------------------------------------------------

# PLOT 1: MISSING VALUES (Using the raw data)
plt.figure(figsize=(10, 5))
sns.barplot(x=df_raw.isna().sum().index, y=df_raw.isna().sum().values, palette="Reds_r")
plt.title("Missing Values Count per Column")
plt.xticks(rotation=45)
plt.show()

# PLOT 2: COUNT PLOT (Taxi Colors)
plt.figure(figsize=(7, 5))
sns.countplot(data=df, x='color', palette=['yellow', 'green'], edgecolor='black')
plt.title("Frequency of Yellow vs Green Taxis")
plt.show()

# PLOT 3: KDE PLOT (Visualizing the T-Test)
plt.figure(figsize=(10, 6))
sns.kdeplot(data=df, x='tip', hue='color', fill=True, palette=['gold', 'seagreen'], common_norm=False)
plt.title("Tip Distribution: Yellow vs Green")
plt.xlim(0, 10) # Focused view
plt.show()

# PLOT 5: SCATTER PLOT (Distance vs Tip)
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='distance', y='tip', hue='color', alpha=0.5, palette=['gold', 'seagreen'])
plt.title("Relationship: Trip Distance vs Tip Amount")
plt.show()

# PLOT 6: HEATMAP (Correlation Matrix)
plt.figure(figsize=(12, 8))
# Select only numerical columns for correlation calculation
numeric_df = df.select_dtypes(include=['float64', 'int64'])
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap (Feature Relationships)")
plt.show()

# Independent T-Test (Yellow vs Green)
yellow_tip = df[df['color'] == 'yellow']['tip']
green_tip = df[df['color'] == 'green']['tip']
t_stat, p_val_ttest = stats.ttest_ind(yellow_tip, green_tip)

# One-Way ANOVA (By Borough)
# Group data for ANOVA
borough_groups = [group['tip'].values for name, group in df.groupby('pickup_borough')]
f_stat, p_val_anova = stats.f_oneway(*borough_groups)

print("\n--- STATISTICAL RESULTS ---")
print(f"T-Test P-value: {p_val_ttest:.5f}")
print(f"ANOVA P-value: {p_val_anova:.5f}")

if p_val_ttest < 0.05:
    print("Conclusion: There is a statistically significant difference in tips based on taxi color.")

df['pickup']

df['pickup'].dt.day_name()

df['pickup'].dt.date

df['pickup'].dt.time

df['pickup'].dt.month_name()

df['pickup'].dt.year

df['pickup'].dt.quarter



df['pickup'].dt.hour

df['pickup'].dt.minute

df['pickup'].dt.second

df['pickup_month']=df['pickup'].dt.month
df.head()

df['pickup_month_name']=df['pickup'].dt.month_name()
df.head()

df['pickup_year']=df['pickup'].dt.year
df.head()

df['pickup_day']=df['pickup'].dt.day
df.head()

df['pickup_day_name']=df['pickup'].dt.day_name()
df.head()

df['pickup_second']=df['pickup'].dt.second
df.head()

df.pickup_month.value_counts()

df['dropoff_monthname']=df['dropoff'].dt.month_name()
df['dropoff_month']=df['dropoff'].dt.month
df['dropoff_year']=df['dropoff'].dt.year
df['dropoff_dayname']=df['dropoff'].dt.day_name()
df['dropoff_hour']=df['dropoff'].dt.hour
df['dropoff_minute']=df['dropoff'].dt.minute
df['dropoff_second']=df['dropoff'].dt.second
df['dropoff_quarter']=df['dropoff'].dt.quarter
df['dropoff_day']=df['dropoff'].dt.day

df.shape

df.head()

from scipy import stats

r, p_val = stats.spearmanr(df['tip'], df['distance'], nan_policy='omit')

print("Spearman Correlation:", r)
print("P-value:", p_val)

alpha = 0.05
if p_val < alpha:
    print("Reject Null Hypothesis: Significant relationship exists")
else:
    print("Fail to Reject Null Hypothesis: No significant relationship")

drop_list = [
    'pickup',
    'dropoff',
    'pickup_month',
    'pickup_day',
    'pickup_year',
    'dropoff_month',
    'dropoff_day',
    'dropoff_minute',
    'dropoff_second',
    'dropoff_quarter'
]

df.drop(columns=drop_list, inplace=True, errors='ignore')

df.head()

from sklearn.preprocessing import LabelEncoder

lb = LabelEncoder()

cat = [
    'color',
    'payment',
    'pickup_zone',
    'dropoff_zone',
    'pickup_borough',
    'dropoff_borough',
    'pickup_dayname',
    'dropoff_dayname'
]

for col in cat:
    if col in df.columns:
        df[col] = lb.fit_transform(df[col])

x=df.drop('tip',axis=1)
y=df.tip

print(x.shape)
print(y.shape)

from sklearn.model_selection import train_test_split
x_train, x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)

print('X train shape : ',x_train. shape)
print('Y train shape : ',y_train.shape)
print('X test shape : ',x_test.shape)
print('Y test shape : ',y_test. shape)

X = df.drop(columns=['tip'])
y = df['tip']

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X.select_dtypes(include='object').columns

from sklearn.preprocessing import LabelEncoder

lb = LabelEncoder()

for col in X.select_dtypes(include='object').columns:
    X[col] = lb.fit_transform(X[col])

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(x_train, y_train)

print(X.dtypes)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Predictions
y_train_pred = model.predict(x_train)
y_test_pred = model.predict(x_test)

# Training metrics
print("TRAIN METRICS")
print("R² Score:", r2_score(y_train, y_train_pred))
print("MAE:", mean_absolute_error(y_train, y_train_pred))
print("MSE:", mean_squared_error(y_train, y_train_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_train, y_train_pred)))

print("\nTEST METRICS")
print("R² Score:", r2_score(y_test, y_test_pred))
print("MAE:", mean_absolute_error(y_test, y_test_pred))
print("MSE:", mean_squared_error(y_test, y_test_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_test_pred)))

import pandas as pd
import numpy as np
dataset = pd.read_csv('/content/Position_Salaries.csv')
print("Shape:", dataset.shape)
print("\nColumns:")
print(dataset.columns)

print("\nFirst 5 rows:")
print(dataset.head())

print("\nDataset Info:")
print(dataset.info())
print("\nMissing values:")
print(dataset.isnull().sum())
dataset = dataset.drop(columns=['Position'], errors='ignore')
X = dataset.iloc[:, :-1].values   # Level
y = dataset.iloc[:, -1].values    # Salary
print("\nX dtype:", X.dtype)
print("y dtype:", y.dtype)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 20% testing data
    random_state=42    # reproducibility
)
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# Training the Linear Regression model on the whole dataset
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, y)

# Training the Polynomial Regression model on the whole dataset
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 4)
X_poly = poly_reg.fit_transform(X)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, y)

print("✅ Models Trained Successfully!")

# Visualising the Linear Regression results
plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg.predict(X), color = 'blue')
plt.title('Truth or Bluff (Linear Regression)')
plt.xlabel('Position Level')
plt.ylabel('Salary')
plt.show()

# Visualising the Polynomial Regression results
plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg_2.predict(poly_reg.fit_transform(X)), color = 'blue')
plt.title('Truth or Bluff (Polynomial Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

#Predicting a new result with Linear Regression
linear_prediction = lin_reg.predict([[6.5]])
print("Linear Regression Prediction for Level 6.5:", linear_prediction)

# Predicting a new result with Polynomial Regression
poly_prediction = lin_reg_2.predict(poly_reg.fit_transform([[6.5]]))
print("Polynomial Regression Prediction for Level 6.5:", poly_prediction)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Predictions on entire dataset (or test set if you have one)
y_pred = lin_reg.predict(X)

# Metrics
mse = mean_squared_error(y, y_pred)
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y, y_pred)

# Print metrics
print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)
print("Root Mean Squared Error (RMSE):", rmse)
print("R2 Score:", r2)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Predict on full dataset using polynomial features
y_poly_pred = lin_reg_2.predict(poly_reg.transform(X))

# Metrics
mse_poly = mean_squared_error(y, y_poly_pred)
mae_poly = mean_absolute_error(y, y_poly_pred)
rmse_poly = np.sqrt(mse_poly)
r2_poly = r2_score(y, y_poly_pred)

# Print metrics
print("Polynomial Regression Metrics")
print("Mean Squared Error (MSE):", mse_poly)
print("Mean Absolute Error (MAE):", mae_poly)
print("Root Mean Squared Error (RMSE):", rmse_poly)
print("R2 Score:", r2_poly)