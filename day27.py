# -*- coding: utf-8 -*-
"""Day27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y6fD8JCK1AVN5BGPZ3RcKHl3JkxB7QQi
"""

import pandas as pd

data=pd.DataFrame({
'age': [25,30,35,40,45],
'salary':[5000,6000,7000,8000,9000],
'country_code': [1,1,1,1,1]
})
data

from sklearn.feature_selection import VarianceThreshold

selector=VarianceThreshold(threshold=0.1)
selected_data=selector.fit_transform(data)

selected_features=data.columns[selector.get_support()]

print("Selected Features:",selected_features)

# Import libraries
import pandas as pd
from sklearn.feature_selection import VarianceThreshold

# Example dataset
data = {
    'Age': [25, 47, 30, 52, 40],
    'Blood_Pressure': [120, 135, 130, 140, 128],
    'Cholesterol': [200, 220, 210, 230, 215],
    'Gender': ['M', 'F', 'M', 'F', 'M'],
    'Country_Code': ['India', 'India', 'India', 'India', 'India'],
    'Hospital_ID': [101, 101, 101, 101, 101],
    'Machine_Version': ['v2.1', 'v2.1', 'v2.1', 'v2.1', 'v2.2'],
}

df = pd.DataFrame(data)
print("Original DataFrame:")
print(df)

# Step 1: Encode categorical variables (simple one-hot encoding)
df_encoded = pd.get_dummies(df, drop_first=True)

# Step 2: Apply VarianceThreshold
# threshold=0 removes features with 0 variance
selector = VarianceThreshold(threshold=0.01)  # 1% variance threshold to remove almost constant features
df_reduced = selector.fit_transform(df_encoded)

# Get remaining feature names
features_kept = df_encoded.columns[selector.get_support()]
print("\nFeatures kept after removing low-variance columns:")
print(features_kept)

# Convert back to DataFrame
df_reduced = pd.DataFrame(df_reduced, columns=features_kept)
print("\nReduced DataFrame:")
print(df_reduced)

import pandas as pd
import numpy as np
data=pd.DataFrame({
    'size_sqft':[1000,1500,2000,2500,3000],
    'titles_count':[100,150,200,250,300],
    'price':[200000,300000,400000,500000,600000]
})
data

correlation_matrix=data.corr()
print(correlation_matrix)

import matplotlib.pyplot as plt
import seaborn as sns
sns. heatmap(correlation_matrix, annot=True, cmap='RdBu_r' )
plt.show();

#Removing highly correlated features (>0.9)

upper=correlation_matrix.where(
np.triu(np.ones(correlation_matrix.shape),k=1).astype(bool)

)

to_drop=[column for column in upper.columns if any(upper[column]>0.9)]

print("Highly Correlated Featured to Drop:", to_drop)

import pandas as pd
import numpy as np

# Example dataset
data = {
    'House_Size': [1200, 1500, 1800, 2000, 2500],
    'Number_of_Tiles': [120, 150, 180, 200, 250],
    'Construction_Cost': [80000, 100000, 120000, 135000, 160000],
    'Bedrooms': [2, 3, 3, 4, 4],
    'Market_Price': [150000, 180000, 210000, 240000, 300000]
}

df = pd.DataFrame(data)

# Step 1: Compute correlation matrix
corr_matrix = df.corr()
print("Correlation Matrix:\n", corr_matrix)

# Step 2: Identify highly correlated features (threshold = 0.9)
threshold = 0.9
# Get upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Find features with correlation above threshold
to_drop = [column for column in upper.columns if any(upper[column].abs() > threshold)]
print("\nHighly correlated features to drop:", to_drop)

# Step 3: Drop them
df_reduced = df.drop(columns=to_drop)
print("\nReduced dataset:\n", df_reduced)

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest, chi2

data = {
    'Gender': ['Male', 'Female', 'Male', 'Male', 'Female'],
    'Marital_Status': ['Yes', 'No', 'Yes', 'Yes', 'No'],
    'Education': ['Graduate', 'Non-Graduate', 'Graduate', 'Graduate', 'Non-Graduate'],
    'Applicant_Income': [5000, 3000, 4000, 6000, 3500],
    'Property_Area': ['Urban', 'Rural', 'Urban', 'Rural', 'Urban'],
    'Favorite_Color': ['Red', 'Blue', 'Green', 'Red', 'Blue'],
    'Loan_Status': ['Approved', 'Rejected', 'Approved', 'Approved', 'Rejected']
}

df = pd.DataFrame(data)

le = LabelEncoder()

for column in df.columns:
    df[column] = le.fit_transform(df[column])

X = df.drop('Loan_Status', axis=1)
y = df['Loan_Status']

selector = SelectKBest(score_func=chi2, k='all')
selector.fit(X, y)

scores = pd.DataFrame({
    'Feature': X.columns,
    'Chi2_Score': selector.scores_
})

print(scores.sort_values(by='Chi2_Score', ascending=False))

import pandas as pd
from sklearn.datasets import load_breast_cancer

#Load the DataSet
data=load_breast_cancer()
X=pd.DataFrame(data.data, columns=data.feature_names)
print (X.head())
print(X.shape)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SequentialFeatureSelector
y=data.target

X_train, X_test,y_train,y_test=train_test_split(
X,y,test_size=0.2,random_state=42

)

model=LogisticRegression(max_iter=50)

from sklearn.feature_selection import SequentialFeatureSelector

sfs = SequentialFeatureSelector(
    model,
    n_features_to_select=5,
    direction='forward'
)

sfs.fit(X_train, y_train)

selected_features = X_train.columns[sfs.get_support()]
print("Selected Features:", selected_features)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

!pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# load the dataset
air_quality = fetch_ucirepo(id=360)
air_quality

# convert to dataframe
data = air_quality.data.features
data.head()

"""Data Preprocessing steps

"""

df=data[['CO(GT)','C6H6(GT)','NOx(GT)','NO2(GT)']]
df.head()

df.isnull().sum()

df.shape

n_estimators=100
contamination=0.01
sample_size=256

from sklearn.ensemble import IsolationForest
model = IsolationForest(n_estimators=n_estimators,
contamination=contamination, max_samples=sample_size,
random_state=42)

model.fit(df)
# Predict anomalies
anomaly_labels = model.predict(df)

# Add anomaly labels to the dataframe
df['anomaly' ] = anomaly_labels
df['anomaly'].value_counts()

# Visualize anomalies - Multiple feature comparisons
fig, axes = plt.subplots(1, 2, figsize=(14,5))

normal = df[df['anomaly' ] == 1]
anomaly = df[df['anomaly' ] == -1]

# Plot 1: CO vs C6H6
axes [0].scatter(normal['CO(GT)'], normal['C6H6(GT)'], color='blue',
label='Normal', alpha=0.6, s=30)
axes [0].scatter(anomaly['CO(GT)'], anomaly['C6H6(GT)'], color='red',
label='Anomaly', alpha=0.8, s=50, marker='x')
axes[0].set_xlabel('CO(GT)')
axes[0].set_ylabel('C6H6(GT)')
axes[0].set_title('CO vs C6H6')
axes [0]. legend ()

# Plot 2: NOx vs NO2
axes[1].scatter(normal['NOx(GT)'], normal['NO2(GT)'], color='blue',
label='Normal', alpha=0.6, s=30)
axes[1].scatter(anomaly['NOx(GT)'], anomaly['NO2(GT)'], color='red',
label='Anomaly', alpha=0.8, s=50, marker='x')
axes[1].set_xlabel('NOx(GT)')
axes[1].set_ylabel('NO2(GT)')
axes[1].set_title('NOx vs NO2')
axes[1].legend()
axes[1].grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"Normal data points: {len(normal)}")

# saved model same folder
import joblib
joblib.dump(model, 'isolation_forest_model.pkl')