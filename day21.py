# -*- coding: utf-8 -*-
"""Day21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xFIRnsP7rEqvWYdpN-tWZ6GNq4Xf6u_w
"""

import numpy as np
import matplotlib.pyplot as plt

# Units consumed
X = np.array([50, 120, 180, 250, 320, 380]).reshape(-1, 1)

# Corresponding electricity bill
y = np.array([500, 1000, 1000, 1800, 2500, 2500])

from sklearn.tree import DecisionTreeRegressor

dt_reg = DecisionTreeRegressor(max_depth=3, random_state=42)
dt_reg.fit(X, y)

X_grid = np.linspace(0, 400, 200).reshape(-1, 1)
y_pred = dt_reg.predict(X_grid)

plt.scatter(X, y, color='blue')
plt.plot(X_grid, y_pred, color='red')
plt.xlabel("Units Consumed")
plt.ylabel("Electricity Bill (₹)")
plt.title("Decision Tree Regression - Electricity Bill Slabs")
plt.show()

units = float(input("Enter electricity units consumed: "))
bill = dt_reg.predict([[units]])

print(f"Predicted Electricity Bill: ₹{bill[0]:.0f}")

"""Predict salary based on years of experience using Decision Tree Regression

Input - Years of Experience

Output - Salary
"""

import numpy as np
import matplotlib.pyplot as plt

X = np.array([1, 2, 3, 4, 5, 7, 8, 9]).reshape(-1, 1)
y = np.array([3, 3.5, 4, 4.2, 6, 6.5, 7, 7.2])

import numpy as np
import matplotlib.pyplot as plt

X = np.array([1, 2, 3, 4, 5, 7, 8, 9]).reshape(-1, 1)
y = np.array([3, 3.5, 4, 4.2, 6, 6.5, 7, 7.2])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.25, random_state=42

)

from sklearn.tree import DecisionTreeRegressor

dt_reg = DecisionTreeRegressor(random_state=42)
dt_reg.fit(X_train, y_train)

X_grid = np.linspace(1, 8, 100).reshape(-1, 1)
y_pred = dt_reg.predict(X_grid)

plt.scatter(X, y, color='blue')
plt.plot(X_grid, y_pred, color='red' )
plt.title("Decision Tree Regression")
plt.xlabel("Years of Experience")
plt.ylabel ("Salary (|P4)")
plt.show()

dt_controlled = DecisionTreeRegressor(max_depth=3)
dt_controlled.fit(X_train, y_train)

y_pred_controlled = dt_controlled.predict(X_grid)

plt.scatter(X, y)
plt.plot(X_grid, y_pred_controlled, color='orange')
plt.title("Controlled Tree (max_depth=3)")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Data
np.random.seed(42)
X = np.arange(1, 21).reshape(-1, 1)
y = np.array([
    10, 12, 15, 18, 20, 25, 30, 28, 35, 40,
    45, 42, 50, 55, 60, 58, 65, 70, 72, 75
])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Fit Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on test set
y_pred = model.predict(X_test)

# Calculate R² and MSE
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(f"R² Score: {r2:.4f}")
print(f"Mean Squared Error: {mse:.4f}")

# Optional: Plot
plt.scatter(X, y, color='blue', label='Actual')
plt.plot(X, model.predict(X), color='red', label='Predicted')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression Fit')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

# Data
np.random.seed(42)
X = np.arange(1, 21).reshape(-1, 1)
y = np.array([
    10, 12, 15, 18, 20, 25, 30, 28, 35, 40,
    45, 42, 50, 55, 60, 58, 65, 70, 72, 75
])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Fit Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on test set
y_pred = rf_model.predict(X_test)

# Calculate R² and MSE
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(f"R² Score: {r2:.4f}")
print(f"Mean Squared Error: {mse:.4f}")

# Optional: Plot predictions
plt.scatter(X, y, color='blue', label='Actual')
plt.scatter(X_test, y_pred, color='red', label='Predicted (Test)')
plt.plot(np.sort(X.flatten()), rf_model.predict(np.sort(X).reshape(-1,1)), color='green', label='Predicted Curve')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Random Forest Regression')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

# =====================
# Data
# =====================
np.random.seed(42)
X = np.arange(1, 21).reshape(-1, 1)
y = np.array([
    10, 12, 15, 18, 20, 25, 30, 28, 35, 40,
    45, 42, 50, 55, 60, 58, 65, 70, 72, 75
])

# =====================
# Train-test split
# =====================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# =====================
# Decision Tree Regressor
# =====================
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)

r2_dt = r2_score(y_test, y_pred_dt)
mse_dt = mean_squared_error(y_test, y_pred_dt)

print("Decision Tree Regression:")
print(f"R² Score: {r2_dt:.4f}")
print(f"Mean Squared Error: {mse_dt:.4f}\n")

# =====================
# Random Forest Regressor
# =====================
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

r2_rf = r2_score(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)

print("Random Forest Regression:")
print(f"R² Score: {r2_rf:.4f}")
print(f"Mean Squared Error: {mse_rf:.4f}\n")

# =====================
# Plotting step-wise predictions
# =====================
X_range = np.arange(X.min(), X.max() + 1).reshape(-1, 1)  # integer steps

plt.scatter(X, y, color='black', label='Actual Data')

plt.step(X_range.flatten(), dt_model.predict(X_range), color='red', where='post', label='Decision Tree')
plt.step(X_range.flatten(), rf_model.predict(X_range), color='green', where='post', label='Random Forest')

plt.xlabel('X')
plt.ylabel('y')
plt.title('Decision Tree vs Random Forest Regression')
plt.legend()
plt.show()

import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv("house_data.csv")

# Select features and target
X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors']]
y = df['price']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(
    n_estimators=200,
    random_state=42,
    max_depth=None
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("R² Score:", r2)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,r2_score
import numpy as np

# Study hours (X) and Marks (y)
X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)
y = np.array([35, 40, 50, 60, 65, 70])

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.3, random_state=42

)

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

y_train_pred = lin_reg.predict(X_train)
y_test_pred = lin_reg.predict(X_test)

print("GOOD FIT (Linear Regression)")
print("Train MSE:", mean_squared_error(y_train, y_train_pred) )
print("Train R2 :", r2_score(y_train, y_train_pred))

plt.scatter(X, y)
plt.plot(X, lin_reg.predict(X), color='green')
plt.title("Good Fit: Marks vs Study Hours")
plt.xlabel("Study Hours")
plt.ylabel("Marks")
plt.show()

poly = PolynomialFeatures(degree=5)

X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly. transform(X_test)

overfit_model = LinearRegression()
overfit_model.fit(X_train_poly, y_train)

y_train_pred_poly = overfit_model. predict(X_train_poly)
y_test_pred_poly = overfit_model.predict(X_test_poly)

print("\nOVERFITTING MODEL (Polynomial Degree = 5)")
print("Train MSE:", mean_squared_error(y_train, y_train_pred_poly))
print("Train R2 :", r2_score(y_train, y_train_pred_poly))

from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=3,
    random_state=42
)

gbr.fit(X_train, y_train)

gbr_pred = gbr.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

mse_gbr = mean_squared_error(y_test, gbr_pred)
r2_gbr = r2_score(y_test, gbr_pred)

print("Gradient Boosting MSE:", mse_gbr)
print("Gradient Boosting R²:", r2_gbr)

import pandas as pd
import numpy as np

# Load dataset (UCI SMS Spam dataset style)
df = pd.read_csv("spam.csv", encoding="latin-1")

# Keep only relevant columns
df = df[['v1', 'v2']]

# Rename columns
df.columns = ['label', 'text']

# Convert labels to binary
df['label'] = df['label'].map({'ham': 0, 'spam': 1})

# Preview data
print(df.head())

import pandas as pd

# Load the uploaded CSV file
df = pd.read_csv('spam.csv', encoding='latin-1')

# Select required columns
df = df[['v1', 'v2']]

# Rename columns
df.columns = ['label', 'text']

# Convert labels to numeric
df['label'] = df['label'].map({'ham': 0, 'spam': 1})

# Display first rows
print(df.head())
df['text_length' ] = df['text' ].apply(len)
df['num_words' ] = df['text'].apply(lambda x: len(x.split()))
df['num_digits'] = df['text'].apply(lambda x: sum(c.isdigit() for c in x))
 # Define features and target
X = df[['text_length', 'num_words', 'num_digits' ]]
y = df['label']
 # Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
from sklearn.preprocessing import StandardScaler

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
from sklearn.naive_bayes import GaussianNB

# Train Gaussian Naive Bayes
gnb = GaussianNB()
gnb.fit(X_train_scaled, y_train)
y_pred_gnb = gnb.predict(X_test_scaled)
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Accuracy score
accuracy = accuracy_score(y_test, y_pred_gnb)
print(f'Accuracy: {accuracy:.4f}')

# Classification report (includes precision, recall, f1-score)
class_report = classification_report(y_test, y_pred_gnb)
print(f'Classification Report:\n{class_report}')

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_gnb)
print(f'Confusion Matrix:\n{conf_matrix}')

# ===============================
# 1. Import Required Libraries
# ===============================
import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


# ===============================
# 2. Load and Preprocess Dataset
# ===============================
df = pd.read_csv("spam.csv", encoding="latin-1")[["v1", "v2"]]
df.columns = ["label", "text"]

# Convert labels to numeric
df["label"] = df["label"].map({"ham": 0, "spam": 1})

print("First 5 rows of dataset:")
print(df.head())


# ===============================
# 3. Check Class Distribution
# ===============================
print("\nClass Distribution:")
print(df["label"].value_counts())


# ===============================
# 4. Text Vectorization (TF-IDF)
# ===============================
vectorizer = TfidfVectorizer(stop_words="english")
X = vectorizer.fit_transform(df["text"])
y = df["label"]


# ===============================
# 5. Train-Test Split
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# ===============================
# 6. Train Naive Bayes Model
# ===============================
model = MultinomialNB()
model.fit(X_train, y_train)


# ===============================
# 7. Predictions
# ===============================
y_pred = model.predict(X_test)


# ===============================
# 8. Model Evaluation
# ===============================
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("\nModel Accuracy:", accuracy)

print("\nClassification Report:\n")
print(classification_report(
    y_test, y_pred,
    target_names=["Not Suspicious (Ham)", "Suspicious (Spam)"]
))

print("\nConfusion Matrix:")
print(cm)

tn, fp, fn, tp = cm.ravel()
print("\nTrue Positives (Spam correctly detected):", tp)
print("False Positives (Ham wrongly flagged):", fp)
print("True Negatives (Ham correctly detected):", tn)
print("False Negatives (Spam missed):", fn)


# ===============================
# 9. Test with Custom Messages
# ===============================
custom_messages = [
    "Congratulations! You have won a free lottery ticket",
    "Can we meet tomorrow to discuss the project?",
    "Urgent! Claim your prize money now",
    "Hello, are you available for a call today?"
]

custom_vec = vectorizer.transform(custom_messages)
predictions = model.predict(custom_vec)

print("\nCustom Message Predictions:\n")
for msg, pred in zip(custom_messages, predictions):
    print("Message:", msg)
    print("Prediction:", "Suspicious" if pred == 1 else "Not Suspicious")
    print("-" * 50)