# -*- coding: utf-8 -*-
"""Day26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10hKOfbJ26_AeCH2lQGdZnoCOGCOfS4MG
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

iris=load_iris()

X=iris.data
y=iris.target

X_scaled=StandardScaler().fit_transform(X)

X.shape

# Import required libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

# Example dataset (You can replace this with your dataset)
data = load_iris()
X = data.data

# Step 1: Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 2: Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Step 3: Scree Plot
plt.figure(figsize=(8, 5))

# Individual explained variance
plt.plot(
    range(1, len(pca.explained_variance_ratio_) + 1),
    pca.explained_variance_ratio_,
    marker='o',
    linestyle='-',
    label='Individual Explained Variance'
)

# Cumulative explained variance
plt.plot(
    range(1, len(pca.explained_variance_ratio_) + 1),
    np.cumsum(pca.explained_variance_ratio_),
    marker='s',
    linestyle='--',
    label='Cumulative Explained Variance'
)

plt.xlabel('Number of Components')
plt.ylabel('Explained Variance Ratio')
plt.title('Scree Plot')
plt.grid(True)
plt.legend()
plt.show()

pca_95=PCA(n_components=0.95)

X_reduced=pca_95.fit_transform(X_scaled)
print(f"Reduced Shape:{X_reduced.shape}")

pca_2d=PCA(n_components=2)
X_2d=pca_2d.fit_transform(X_scaled)

plt.figure(figsize=(8,6))
sns.scatterplot(x=X_2d[:,0],y=X_2d[ : , 1], hue=iris. target_names[y], palette='Set2' )
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('2D Projection of Iris Dataset')
plt.grid(True)
plt.show()

# ===============================
# IMPORT LIBRARIES
# ===============================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# ===============================
# LOAD DATASET
# ===============================
df = pd.read_csv("marketing_campaign.csv", sep='\t')

print("Dataset Shape:", df.shape)
print(df.head())

# ===============================
# SELECT NUMERICAL BEHAVIOR FEATURES
# ===============================

numerical_features = df.select_dtypes(include=[np.number])

print("Numerical Features:")
print(numerical_features.columns)

# ===============================
# HANDLE MISSING VALUES
# ===============================
numerical_features = numerical_features.fillna(numerical_features.mean())

# ===============================
# STANDARDIZE DATA
# ===============================
scaler = StandardScaler()
X_scaled = scaler.fit_transform(numerical_features)

print("Data Standardized Successfully")

# ===============================
# APPLY PCA
# ===============================
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# ===============================
# EXPLAINED VARIANCE PLOT
# ===============================
plt.figure()
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),
         pca.explained_variance_ratio_,
         marker='o')
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),
         np.cumsum(pca.explained_variance_ratio_),
         marker='s',
         linestyle='--')
plt.xlabel("Number of Components")
plt.ylabel("Explained Variance Ratio")
plt.title("Scree Plot")
plt.grid(True)
plt.show()

# ===============================
# FIND COMPONENTS FOR 90% VARIANCE
# ===============================
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)
n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1

print("Number of components to retain 90% variance:", n_components_90)

# ===============================
# REDUCE DATASET
# ===============================
pca_final = PCA(n_components=n_components_90)
X_reduced = pca_final.fit_transform(X_scaled)

print("Reduced Dataset Shape:", X_reduced.shape)

# ===============================
# PCA LOADINGS (Feature Contribution)
# ===============================
loadings = pd.DataFrame(
    pca.components_.T,
    columns=[f"PC{i+1}" for i in range(len(numerical_features.columns))],
    index=numerical_features.columns
)

# Top contributors to PC1
print("\nTop Contributors to PC1:")
print(loadings["PC1"].abs().sort_values(ascending=False).head())

# Top contributors to PC2
print("\nTop Contributors to PC2:")
print(loadings["PC2"].abs().sort_values(ascending=False).head())

plt.figure()
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("Customers in PCA Space")
plt.grid(True)
plt.show()

kmeans_original = KMeans(n_clusters=4, random_state=42)

start = time.time()
kmeans_original.fit(X_scaled)
original_time = time.time() - start

original_silhouette = silhouette_score(X_scaled, kmeans_original.labels_)

print("Original Data Silhouette Score:", original_silhouette)
print("Original Data Computation Time:", original_time)

kmeans_pca = KMeans(n_clusters=4, random_state=42)

start = time.time()
kmeans_pca.fit(X_reduced)
pca_time = time.time() - start

pca_silhouette = silhouette_score(X_reduced, kmeans_pca.labels_)

print("PCA Data Silhouette Score:", pca_silhouette)
print("PCA Data Computation Time:", pca_time)

plt.figure()
plt.scatter(X_reduced[:, 0],
            X_reduced[:, 1],
            c=kmeans_pca.labels_)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("K-Means Clusters in PCA Space")
plt.grid(True)
plt.show()

