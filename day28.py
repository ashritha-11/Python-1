# -*- coding: utf-8 -*-
"""Day28.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RLO1AAY32mmmjoG5k08BeeXQDNrqLvP3
"""

import numpy as np
from sklearn.linear_model import LogisticRegression

# Data
X = np.array([1,2,3,4]).reshape(-1,1)
y = np.array([0,0,1,1])

# Model
model = LogisticRegression()
model.fit(X, y)

# Prediction
print("Prediction for 3 hours:", model.predict([[3]]))
print("Prediction for 5 hours:", model.predict([[5]]))

w=0.0
b=0.0
learning_rate=1
epochs=5

print("Initial Weight:",w)
print("Initial Bias:",b)

"""Define Activation Function"""

def step_function(z):
  return 1 if z>=0 else 0

"""Training Loop"""

import numpy as np

X = np.array([1, 2, 3, 4])
y = np.array([0, 0, 1, 1])

w = 0.0
b = 0.0
learning_rate = 0.1
epochs = 5

def step_function(z):
    return 1 if z >= 0 else 0

for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}")

    for i in range(len(X)):
        z = w * X[i] + b
        y_pred = step_function(z)
        error = y[i] - y_pred

        w = w + learning_rate * error * X[i]
        b = b + learning_rate * error

        print(f"Input: {X[i]}, Predicted: {y_pred}, Actual: {y[i]}, w: {w:.2f}, b: {b:.2f}")

print("\nFinal Weight:", w)
print("Final Bias:", b)

"""Testing the model"""

test_value=2.9
z=w*test_value+b
prediction=step_function(z)

print(f"For study hours {test_value}, Prediction", prediction)

import numpy as np
import matplotlib.pyplot as plt

# -------------------------
# Dataset (OR Logic)
# -------------------------
X = np.array([
    [0, 0],
    [1, 0],
    [0, 1],
    [1, 1]
])

y = np.array([0, 1, 1, 1])  # OR function

# -------------------------
# Perceptron Parameters
# -------------------------
weights = np.zeros(2)
bias = 0
learning_rate = 0.1
epochs = 10

# -------------------------
# Step Function
# -------------------------
def step_function(z):
    return 1 if z >= 0 else 0

# -------------------------
# Training
# -------------------------
for epoch in range(epochs):
    for i in range(len(X)):
        linear_output = np.dot(X[i], weights) + bias
        prediction = step_function(linear_output)

        error = y[i] - prediction

        weights += learning_rate * error * X[i]
        bias += learning_rate * error

print("Final Weights:", weights)
print("Final Bias:", bias)

# -------------------------
# Decision Boundary
# -------------------------
# Equation: w1*x1 + w2*x2 + b = 0
print("\nDecision Boundary Equation:")
print(f"{weights[0]}*x1 + {weights[1]}*x2 + {bias} = 0")

# -------------------------
# Plot Decision Boundary
# -------------------------
plt.figure()

# Plot points
for i in range(len(X)):
    if y[i] == 1:
        plt.scatter(X[i][0], X[i][1], marker='o')
    else:
        plt.scatter(X[i][0], X[i][1], marker='x')

# Plot boundary line
x_values = np.linspace(-0.5, 1.5, 100)
y_values = -(weights[0] * x_values + bias) / weights[1]

plt.plot(x_values, y_values)

plt.xlim(-0.5, 1.5)
plt.ylim(-0.5, 1.5)
plt.xlabel("Suspicious Keywords (x1)")
plt.ylabel("Unknown Sender (x2)")
plt.title("Perceptron Decision Boundary (Spam Detection)")
plt.show()

