# -*- coding: utf-8 -*-
"""Day24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SrHggXipw8TEwZBOjiDgJxAxGHh1zX5Y

Stacking:
"""

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

df=pd.read_csv("Social_Network_Ads.csv")
df.head()

df.isnull().sum()

x=df.iloc[:,[2,3]].values
y=df.iloc[:,-1].values

X_train,X_test,y_train,y_test=train_test_split(
    x,y,test_size=0.2,random_state=42
)

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""Define Base Models"""

base_models = [
('Ir', LogisticRegression()),
('dt', DecisionTreeClassifier(max_depth=3)),
('knn', KNeighborsClassifier(n_neighbors=5))
]

"""Define Meta Model

Meta model decides how to combine predictions
"""

meta_model=LogisticRegression()

"""Building Stacking Classifier"""

classifier = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5

)

classifier.fit(X_train, y_train)

acc=accuracy_score(y_test,classifier.predict(X_test))
print(acc)

con=confusion_matrix(y_test,classifier.predict(X_test))
print(con)

df = pd.read_csv('/content/loan_approved.csv')
df.head()

df.info()
df.isnull().sum()

y = df['Loan_Status (Approved)']
X = df.drop(['Loan_ID', 'Loan_Status (Approved)'], axis=1)

numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

print("Target variable (y):")
print(y.head())
print("\nNumerical Features:")
print(numerical_features)
print("\nCategorical Features:")
print(categorical_features)

print(X['Dependents'].value_counts())
print(X['Dependents'].dtype)

X['Dependents'] = X['Dependents'].replace('3+', '3').astype(float)

# Impute missing numerical values with the median
for col in numerical_features:
    if X[col].isnull().any():
        median_val = X[col].median()
        X[col].fillna(median_val, inplace=True)

# Impute missing categorical values with the mode
for col in categorical_features:
    if X[col].isnull().any():
        mode_val = X[col].mode()[0]
        X[col].fillna(mode_val, inplace=True)

# Verify that all missing values have been handled
print("Missing values after imputation:")
print(X.isnull().sum())

X['Dependents'] = X['Dependents'].replace('3+', '3').astype(float)

# Re-identify numerical and categorical features after Dependents conversion
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

# Impute missing numerical values with the median
for col in numerical_features:
    if X[col].isnull().any():
        median_val = X[col].median()
        X[col] = X[col].fillna(median_val)

# Impute missing categorical values with the mode
for col in categorical_features:
    if X[col].isnull().any():
        mode_val = X[col].mode()[0]
        X[col] = X[col].fillna(mode_val)

# Verify that all missing values have been handled
print("Missing values after imputation:")
print(X.isnull().sum())

from sklearn.preprocessing import LabelEncoder

# Apply one-hot encoding to categorical features in X
X = pd.get_dummies(X, columns=categorical_features, drop_first=True)

# Create an instance of LabelEncoder
le = LabelEncoder()

# Fit and transform the target variable y
y = le.fit_transform(y)

print("Head of X after encoding:")
print(X.head())

print("\nHead of y after encoding:")
print(y[:5])

print("\nShape of X after encoding:")
print(X.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

lr_model = LogisticRegression()
dt_model = DecisionTreeClassifier(random_state=42)
knn_model = KNeighborsClassifier(n_neighbors=5)

print("Logistic Regression model initialized.")
print("Decision Tree Classifier model initialized.")
print("K-Nearest Neighbors Classifier model initialized.")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Initialize StandardScaler
scaler = StandardScaler()

# Scale the features
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Re-initialize the Logistic Regression model with an increased max_iter to prevent convergence warning
lr_model = LogisticRegression(max_iter=1000)

lr_model.fit(X_train_scaled, y_train)
lr_pred = lr_model.predict(X_test_scaled)
lr_accuracy = accuracy_score(y_test, lr_pred)

print(f"Logistic Regression Accuracy: {lr_accuracy:.4f}")

dt_model.fit(X_train_scaled, y_train)
dt_pred = dt_model.predict(X_test_scaled)
dt_accuracy = accuracy_score(y_test, dt_pred)

print(f"Decision Tree Accuracy: {dt_accuracy:.4f}")

knn_model.fit(X_train_scaled, y_train)
knn_pred = knn_model.predict(X_test_scaled)
knn_accuracy = accuracy_score(y_test, knn_pred)

print(f"K-Nearest Neighbors Accuracy: {knn_accuracy:.4f}")

print(f"Logistic Regression Accuracy: {lr_accuracy:.4f}")
print(f"Decision Tree Accuracy: {dt_accuracy:.4f}")
print(f"K-Nearest Neighbors Accuracy: {knn_accuracy:.4f}")

accuracies = {
    'Logistic Regression': lr_accuracy,
    'Decision Tree': dt_accuracy,
    'K-Nearest Neighbors': knn_accuracy
}

best_model = max(accuracies, key=accuracies.get)
best_accuracy = accuracies[best_model]

print(f"\nThe best performing model among the base models is {best_model} with an accuracy of {best_accuracy:.4f}.")
print("\nExplanation for superior performance (General):\nLogistic Regression performs well when the data is linearly separable and the features have a clear relationship with the target variable, especially after scaling. Decision Trees can sometimes overfit if not constrained (e.g., by max_depth), leading to lower generalization accuracy. KNN's performance heavily depends on the distance metric and the choice of 'k', and it can be sensitive to noisy data and irrelevant features. In this case, Logistic Regression might have found a better linear boundary in the scaled feature space, or the dataset characteristics are more suitable for a linear model than tree-based or instance-based approaches without further hyperparameter tuning for the latter.")

base_models = [
    ('lr', lr_model),
    ('dt', dt_model),
    ('knn', knn_model)
]

print("Base models for StackingClassifier prepared:")
for name, model in base_models:
    print(f"- {name}: {model.__class__.__name__}")

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

print("Accuracy Scores:")
print(f"  Logistic Regression: {lr_accuracy:.4f}")
print(f"  Stacking Classifier: {stacking_accuracy:.4f}")

# Confusion Matrix for Logistic Regression
lr_cm = confusion_matrix(y_test, lr_pred)
print("\nConfusion Matrix for Logistic Regression:")
print(lr_cm)

# Confusion Matrix for Stacking Classifier
stacking_cm = confusion_matrix(y_test, stacking_pred)
print("\nConfusion Matrix for Stacking Classifier:")
print(stacking_cm)

# Visualize Confusion Matrices
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_title('Logistic Regression Confusion Matrix')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

sns.heatmap(stacking_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])
axes[1].set_title('Stacking Classifier Confusion Matrix')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

plt.tight_layout()
plt.show()

meta_model = LogisticRegression(max_iter=1000)

print("Meta-model (Logistic Regression) initialized.")

from sklearn.ensemble import StackingClassifier

# Instantiate the StackingClassifier
stacking_classifier = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5
)

# Train the StackingClassifier
stacking_classifier.fit(X_train_scaled, y_train)

print("StackingClassifier instantiated and trained successfully.")

stacking_pred = stacking_classifier.predict(X_test_scaled)
stacking_accuracy = accuracy_score(y_test, stacking_pred)

print(f"Stacking Classifier Accuracy: {stacking_accuracy:.4f}")

print(f"Logistic Regression Accuracy: {lr_accuracy:.4f}")
print(f"Decision Tree Accuracy: {dt_accuracy:.4f}")
print(f"K-Nearest Neighbors Accuracy: {knn_accuracy:.4f}")
print(f"Stacking Classifier Accuracy: {stacking_accuracy:.4f}")

import pandas as pd

# Load dataset
df = pd.read_csv("Wholesale customers data.csv")

# Basic inspection
print(df.head())
print(df.info())
print(df.describe())
df_spending = df.drop(columns=['Channel', 'Region'])

features = [
    'Fresh', 'Milk', 'Grocery',
    'Frozen', 'Detergents_Paper', 'Delicassen'
]

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_spending)

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8,5))
plt.plot(range(2,11), inertia, marker='o')
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.title("Elbow Method for Optimal K")
plt.show()

kmeans = KMeans(n_clusters=3, random_state=42)
df['Cluster'] = kmeans.fit_predict(scaled_data)

print(df.head())

plt.figure(figsize=(8,6))

plt.scatter(
    df['Grocery'],
    df['Detergents_Paper'],
    c=df['Cluster'],
    cmap='viridis',
    alpha=0.6
)

centers = scaler.inverse_transform(kmeans.cluster_centers_)
plt.scatter(
    centers[:,2], centers[:,4],
    color='red', marker='X', s=200,
    label='Cluster Centers'
)

plt.xlabel("Grocery Spending")
plt.ylabel("Detergents & Paper Spending")
plt.title("Customer Clusters Visualization")
plt.legend()
plt.show()

cluster_profile = df.groupby('Cluster')[features].mean()
print(cluster_profile)

kmeans_alt = KMeans(n_clusters=3, random_state=99)
df['Cluster_New'] = kmeans_alt.fit_predict(scaled_data)

comparison = (df['Cluster'] == df['Cluster_New']).value_counts()
print(comparison)