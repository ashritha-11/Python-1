# -*- coding: utf-8 -*-
"""Day19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xLeAbgcjflAwRYRxVKo1WkgTIC3Ou8tG
"""

import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("CarPrice_Assignment.csv")

# Correct column names
X = df[['enginesize']]
y = df['price']

# Scatter plot
plt.scatter(X, y)
plt.xlabel("Engine Size")
plt.ylabel("Car Price")
plt.title("Engine Size vs Car Price")
plt.show()

from sklearn.linear_model import LinearRegression

linear_model = LinearRegression()
linear_model.fit(X, y)

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

poly_model = LinearRegression()
poly_model.fit(X_poly, y)

# Create input as DataFrame with correct column name
engine_200 = pd.DataFrame([[200]], columns=['enginesize'])

# Predictions
linear_pred = linear_model.predict(engine_200)
poly_pred = poly_model.predict(poly.transform(engine_200))

print("Linear Regression Prediction:", linear_pred[0])
print("Polynomial Regression Prediction:", poly_pred[0])

poly3 = PolynomialFeatures(degree=3)
X_poly3 = poly3.fit_transform(X)

poly_model3 = LinearRegression()
poly_model3.fit(X_poly3, y)

import numpy as np
import matplotlib.pyplot as plt

X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)

# Convert to DataFrame with correct feature name
X_range_df = pd.DataFrame(X_range, columns=['enginesize'])

plt.scatter(X, y, label="Actual Data")

# Linear model
plt.plot(X_range_df,
         linear_model.predict(X_range_df),
         label="Linear Model", linewidth=2)

# Polynomial model
plt.plot(X_range_df,
         poly_model.predict(poly.transform(X_range_df)),
         label="Polynomial Model (Degree 2)", linewidth=2)

plt.xlabel("Engine Size")
plt.ylabel("Car Price")
plt.title("Linear vs Polynomial Regression")
plt.legend()
plt.show()

print("Engine Size = 200")
print("Linear Model Price:", linear_pred[0])
print("Polynomial Model Price:", poly_pred[0])

from sklearn.metrics import r2_score, mean_squared_error
y_pred_linear = linear_model.predict(X)
y_pred_poly = poly_model.predict(poly.transform(X))

r2_linear = r2_score(y, y_pred_linear)
r2_poly = r2_score(y, y_pred_poly)

print("Linear Regression R²:", r2_linear)
print("Polynomial Regression R²:", r2_poly)

mse_linear = mean_squared_error(y, y_pred_linear)
mse_poly = mean_squared_error(y, y_pred_poly)

print("Linear Regression MSE:", mse_linear)
print("Polynomial Regression MSE:", mse_poly)

poly = PolynomialFeatures(degree=3)
X_poly = poly.fit_transform(X)

poly_reg = LinearRegression()
poly_reg.fit(X_poly, y)

# Sort data for smooth curve
X_sorted = X.sort_values(by='enginesize')
X_sorted_poly = poly.transform(X_sorted)

plt.figure(figsize=(8,6))

# Actual data points
plt.scatter(X, y, label="Actual Data")

# Linear model
plt.plot(X_sorted, linear_model.predict(X_sorted), label="Linear Model")

# Polynomial model
plt.plot(X_sorted, poly_reg.predict(X_sorted_poly), label="Polynomial Model (Degree 3)")

plt.xlabel("Engine Size")
plt.ylabel("Car Price")
plt.title("Linear vs Curved Model: Engine Size vs Price")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Example dataset: Study Hours vs. Pass/Fail (Binary Classification)
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]) # Study Hours
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1]) # 0 = Fail, 1 = Pass

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Predictions:", y_pred)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Classification Report
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Generate values for plotting
X_range = np.linspace(0, 12, 100).reshape(-1, 1)
y_prob = model.predict_proba(X_range)[:, 1]  # Probability of class 1

# Plot
plt.scatter(X, y, color="red", label="Actual Data")
plt.plot(X_range, y_prob, color="blue", label="Logistic Regression Curve")
plt.xlabel("Study Hours")
plt.ylabel("Probability of Passing")  # Fixed typo here
plt.title("Logistic Regression - Study Hours vs. Pass/Fail")
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

ConfusionMatrixDisplay.from_predictions(
y_test,
y_pred,
cmap="Blues",
values_format="d"
)

plt.title("Confusion Matrix")
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Step 1: Load the dataset
data = pd.read_csv("heart.csv")
print("First 5 rows:\n", data.head())
print("\nDataset Info:")
print(data.info())
print("\nMissing values:\n", data.isnull().sum())

# Step 2: Separate features and target
X = data.drop("target", axis=1)  # assuming 'target' column is the label
y = data["target"]

# Step 3: Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
print("\nCategorical Columns:", categorical_cols)
print("Numerical Columns:", numerical_cols)

# Step 4: Preprocessing pipelines
# Numerical: Standard scaling
# Categorical: One-hot encoding
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(drop='first'), categorical_cols)  # drop='first' avoids dummy variable trap
    ]
)

# Step 5: Split dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 6: Apply preprocessing
X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)

print("\nPreprocessed feature shapes:")
print("X_train:", X_train.shape)
print("X_test:", X_test.shape)

# Heart Disease Prediction using Logistic Regression (with Probability Plot)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
import numpy as np

# Step 1: Load the dataset
df = pd.read_csv("heart.csv")
print("First 5 rows:\n", df.head())

# Step 2: Define features (X) and target (y)
X = df.drop("target", axis=1)
y = df["target"]

# Step 3: Split into training and testing sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Step 4: Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("\nCategorical Columns:", categorical_cols)
print("Numerical Columns:", numerical_cols)

# Step 5: Preprocessing pipelines
if categorical_cols:
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_cols),
            ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)
        ]
    )
else:
    preprocessor = ColumnTransformer(
        transformers=[('num', StandardScaler(), numerical_cols)]
    )

# Apply preprocessing
X_train_scaled = preprocessor.fit_transform(X_train)
X_test_scaled = preprocessor.transform(X_test)

# Step 6: Train Logistic Regression model
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train_scaled, y_train)

# Step 7: Make predictions
y_pred = model.predict(X_test_scaled)
y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probability for ROC curve

# Step 8: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy: {accuracy:.2f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# Step 9: Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0,1], [0,1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# Step 10: Probability Plot vs Age (or any feature)
# Choose a feature to visualize probability
feature_name = 'age'  # Change to any numeric feature you want
feature_index = numerical_cols.index(feature_name)

# Generate range of values for the selected feature
feature_values = np.linspace(X[feature_name].min(), X[feature_name].max(), 100).reshape(-1, 1)

# Copy other features as mean values
X_mean = X[numerical_cols].mean().values.reshape(1, -1)
X_prob = np.repeat(X_mean, 100, axis=0)

# Replace the selected feature with generated values
X_prob[:, feature_index] = feature_values.flatten()

# If there are categorical features, add zeros for encoded columns
if categorical_cols:
    X_prob_cat = np.zeros((100, len(preprocessor.transformers_[1][1].get_feature_names_out())))
    X_prob_full = np.hstack([X_prob, X_prob_cat])
else:
    X_prob_full = X_prob

# Scale using preprocessor
X_prob_scaled = preprocessor.transform(pd.DataFrame(X_prob_full, columns=numerical_cols + categorical_cols if categorical_cols else numerical_cols))

# Predict probability
y_prob_feature = model.predict_proba(X_prob_scaled)[:, 1]

# Plot probability vs feature
plt.figure(figsize=(8,6))
plt.plot(feature_values, y_prob_feature, color='blue', lw=2)
plt.scatter(X[feature_name], y, color='red', alpha=0.5, label='Actual Data')
plt.xlabel(feature_name.capitalize())
plt.ylabel('Probability of Heart Disease')
plt.title(f'Probability of Heart Disease vs {feature_name.capitalize()}')
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix (raw numbers):\n", cm)

# Optional: Plot confusion matrix as heatmap
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Telco Customer Churn Prediction using Logistic Regression

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

# ----------------------------------------------------
# Step 1: Load Dataset
# ----------------------------------------------------
df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
print(df.head())
print(df.info())

# ----------------------------------------------------
# Step 2: Data Cleaning
# ----------------------------------------------------

# Convert TotalCharges to numeric (it has spaces)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Drop rows with missing values
df.dropna(inplace=True)

# Drop customerID (not useful for prediction)
df.drop('customerID', axis=1, inplace=True)

# Encode target variable
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

# ----------------------------------------------------
# Step 3: Define Features and Target
# ----------------------------------------------------
X = df.drop('Churn', axis=1)
y = df['Churn']

# ----------------------------------------------------
# Step 4: Train-Test Split
# ----------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------------------------------
# Step 5: Identify Column Types
# ----------------------------------------------------
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("Categorical Columns:", categorical_cols)
print("Numerical Columns:", numerical_cols)

# ----------------------------------------------------
# Step 6: Preprocessing (Scaling + Encoding)
# ----------------------------------------------------
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)
    ]
)

X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# ----------------------------------------------------
# Step 7: Train Logistic Regression Model
# ----------------------------------------------------
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train_processed, y_train)

# ----------------------------------------------------
# Step 8: Predictions
# ----------------------------------------------------
y_pred = model.predict(X_test_processed)
y_pred_prob = model.predict_proba(X_test_processed)[:, 1]

# ----------------------------------------------------
# Step 9: Evaluation
# ----------------------------------------------------
print("\nAccuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ----------------------------------------------------
# Step 10: Correct Confusion Matrix
# ----------------------------------------------------
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,4))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=['No Churn', 'Churn'],
    yticklabels=['No Churn', 'Churn']
)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Telco Customer Churn')
plt.show()

# ----------------------------------------------------
# Step 11: ROC Curve
# ----------------------------------------------------
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(7,5))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0,1], [0,1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Telco Customer Churn')
plt.legend()
plt.show()